# AIRI项目模型驱动原理深度解析

AIRI（Artificial Intelligence Realized Interface）是一个开源项目，旨在创建像Neuro-sama那样的AI虚拟角色（VTuber）。其中一个重要组成部分就是虚拟角色的模型驱动系统，支持Live2D和VRM两种主流的虚拟角色格式。本文将深入解析AIRI项目中模型驱动的实现原理。

## 项目架构概述

AIRI项目使用Vue.js和TypeScript构建，通过Web技术实现跨平台支持。在模型驱动方面，它支持两种主要的虚拟角色格式：

1. **Live2D**：2D模型，使用Live2D Cubism技术
2. **VRM**：3D模型，基于glTF扩展的开放格式

## Live2D模型驱动原理

### 模型加载与初始化

Live2D模型的驱动主要通过 pixi-live2d-display 库实现，该库基于PixiJS渲染引擎。在AIRI中，模型加载流程如下：

```typescript
const modelInstance = new Live2DModel<PixiLive2DInternalModel>()
if (modelSrcRef.value.startsWith('blob:')) {
  const res = await fetch(modelSrcRef.value)
  const blob = await res.blob()
  await Live2DFactory.setupLive2DModel(modelInstance, [new File([blob], 'model.zip')], { autoInteract: false })
}
else {
  await Live2DFactory.setupLive2DModel(modelInstance, modelSrcRef.value, { autoInteract: false })
}
```

模型加载完成后，系统会解析模型中的动作定义，并建立情绪与动作的映射关系：

```typescript
availableMotions.value.forEach((motion) => {
  if (motion.motionName in Emotion) {
    motionMap.value[motion.fileName] = motion.motionName
  }
  else {
    motionMap.value[motion.fileName] = EmotionNeutralMotionName
  }
})
```

### 参数控制与动画

Live2D模型的核心在于参数控制。AIRI通过以下方式控制模型的关键参数：

1. **嘴部同步**：根据语音音频的音量大小控制`ParamMouthOpenY`参数，实现口型同步
2. **眼部控制**：通过`ParamEyeBallX`和`ParamEyeBallY`参数控制眼球运动
3. **表情动作**：通过播放不同的motion文件实现表情变化

```typescript
watch(mouthOpenSize, value => getCoreModel().setParameterValueById('ParamMouthOpenY', value))
watch(currentMotion, value => setMotion(value.group, value.index))
```

### 眼部追踪与自动眨眼

AIRI实现了智能的眼部追踪系统，让模型能够跟随鼠标或用户的位置：

```typescript
watch(focusAt, (value) => {
  if (!model.value)
    return
  if (props.disableFocusAt)
    return
  model.value.focus(value.x, value.y)
})
```

同时，系统还实现了自动眨眼功能，通过hook模型的更新函数来手动更新眼部参数：

```typescript
const hookedUpdate = motionManager.update as (model: CubismModel, now: number) => boolean
motionManager.update = function (model: CubismModel, now: number) {
  // ... 更新逻辑
  internalModel.eyeBlink.updateParameters(model, (now - lastUpdateTime.value) / 1000)
  // ...
}
```

## VRM模型驱动原理

### 表情系统

VRM模型使用表情系统（Expression）来实现面部表情变化。AIRI定义了丰富的情绪状态：

```typescript
const emotionStates = new Map<string, EmotionState>([
  ['happy', {
    expression: [
      { name: 'happy', value: 1.0, duration: 0.3 },
      { name: 'aa', value: 0.3 },
    ],
    blendDuration: 0.3,
  }],
  // ... 其他情绪状态
])
```

通过[setEmotion](packages\stage-ui\src\composables\vrm\expression.ts#L76-L108)方法可以设置特定表情，并支持表情之间的平滑过渡：

```typescript
const setEmotion = (emotionName: string) => {
  // 设置表情值
  for (const expr of emotionState.expression || []) {
    vrm.expressionManager?.setValue(expr.name, expr.value)
  }
}
```

### 表情混合与过渡

VRM模型支持多个表情参数的混合，AIRI通过插值算法实现平滑的过渡效果：

```typescript
const lerp = (start: number, end: number, t: number): number => {
  return start + (end - start) * t
}

const easeInOutCubic = (t: number): number => {
  return t < 0.5 ? 4 * t * t * t : 1 - (-2 * t + 2) ** 3 / 2
}

// 在更新循环中进行插值
const currentValue = lerp(
  startValue,
  targetValue,
  easeInOutCubic(transitionProgress.value),
)
vrm.expressionManager?.setValue(exprName, currentValue)
```

## 情绪识别与响应系统

AIRI通过分析AI生成的文本内容来识别情绪，并驱动模型做出相应的表情和动作：

```typescript
const emotionsQueue = createQueue<Emotion>({
  handlers: [
    async (ctx) => {
      if (stageModelRenderer.value === 'vrm') {
        const value = EMOTION_VRMExpressionName_value[ctx.data]
        if (!value)
          return
        await vrmViewerRef.value!.setExpression(value)
      }
      else if (stageModelRenderer.value === 'live2d') {
        currentMotion.value = { group: EMOTION_EmotionMotionName_value[ctx.data] }
      }
    },
  ],
})
```

系统会检测文本中的特殊标记（如`<|EMOTE_HAPPY|>`），然后触发相应的情绪表达。

## 音频驱动的口型同步

在语音合成过程中，AIRI实时分析音频数据并驱动模型口型：

```typescript
function getVolumeWithMinMaxNormalizeWithFrameUpdates() {
  requestAnimationFrame(getVolumeWithMinMaxNormalizeWithFrameUpdates)
  if (!nowSpeaking.value)
    return
  mouthOpenSize.value = calculateVolume(audioAnalyser.value!, 'linear')
}
```

通过Web Audio API分析音频音量，并将结果映射到模型的嘴部参数，实现自然的口型同步效果。

## 总结

AIRI项目的模型驱动系统通过以下关键技术实现：

1. **多格式支持**：同时支持Live2D和VRM两种主流虚拟角色格式
2. **参数化控制**：通过控制模型参数实现精细的表情和动作控制
3. **智能追踪**：实现眼部追踪和自动眨眼等自然行为
4. **情绪识别**：通过文本分析驱动模型情绪表达
5. **音频同步**：实时音频分析实现口型同步

这套系统使得AIRI能够创建出表现力丰富、交互自然的虚拟角色，为用户带来更加生动和沉浸的体验。通过开源的方式，AIRI为虚拟角色技术的发展和普及做出了重要贡献。
